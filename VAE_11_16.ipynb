{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kcurr01/HUT_Research/blob/main/VAE_11_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8iJEJRDVWqt"
      },
      "source": [
        "---\n",
        "Instalation \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du8slAkNSRBk",
        "outputId": "fb94f27a-fccf-4d77-b801-a3b16bda0adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting captum\n",
            "  Downloading captum-0.5.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from captum) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->captum) (4.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->captum) (1.15.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (4.13.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82830 sha256=d51e2806fb08fcd1d0e7d1ceca02851067f6973c36a94b65c961d55907d38f90\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55509 sha256=139bf2e13a7cce7415a32351fecc31b3fec09eda8e3f734685a0efd94a4d3db6\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/bc/eb/974072a56a7082a302f8b4be1ad6d21bf5019235c2eff65928\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.8 umap-learn-0.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datashader\n",
            "  Downloading datashader-0.14.2-py2.py3-none-any.whl (18.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.2 MB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from datashader) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from datashader) (1.7.3)\n",
            "Collecting datashape>=0.5.1\n",
            "  Downloading datashape-0.5.2.tar.gz (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyct>=0.4.5 in /usr/local/lib/python3.7/dist-packages (from datashader) (0.4.8)\n",
            "Requirement already satisfied: colorcet>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from datashader) (3.0.1)\n",
            "Requirement already satisfied: xarray>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from datashader) (0.20.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from datashader) (2.23.0)\n",
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.7/dist-packages (from datashader) (2022.2.0)\n",
            "Requirement already satisfied: param>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from datashader) (1.12.2)\n",
            "Requirement already satisfied: pandas>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from datashader) (1.3.5)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.7/dist-packages (from datashader) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from datashape>=0.5.1->datashader) (1.21.6)\n",
            "Requirement already satisfied: multipledispatch>=0.4.7 in /usr/local/lib/python3.7/dist-packages (from datashape>=0.5.1->datashader) (0.6.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from datashape>=0.5.1->datashader) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from multipledispatch>=0.4.7->datashape>=0.5.1->datashader) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51->datashader) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.51->datashader) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51->datashader) (0.39.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.1->datashader) (2022.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7 in /usr/local/lib/python3.7/dist-packages (from xarray>=0.9.6->datashader) (4.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader) (1.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader) (6.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader) (2022.10.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader) (21.3)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader) (0.12.0)\n",
            "Requirement already satisfied: distributed==2022.02.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader) (2022.2.0)\n",
            "Requirement already satisfied: bokeh>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader) (2.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader) (2.11.3)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader) (2.4.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader) (7.1.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader) (2.2.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader) (6.0.4)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader) (5.4.8)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader) (1.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->dask[complete]->datashader) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->dask[complete]->datashader) (3.0.9)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask[complete]->datashader) (1.0.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed==2022.02.0->dask[complete]->datashader) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.51->datashader) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->datashader) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->datashader) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->datashader) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->datashader) (3.0.4)\n",
            "Building wheels for collected packages: datashape\n",
            "  Building wheel for datashape (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datashape: filename=datashape-0.5.2-py3-none-any.whl size=59438 sha256=c54597b45495b7eab205edd39568635cee9cbc434ef358c8c962c5716647ee22\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b7/80/333a5c3312ed4cd54f5d5b869868c14e0c6002cb5c7238b52d\n",
            "Successfully built datashape\n"
          ]
        }
      ],
      "source": [
        "!pip install captum\n",
        "!pip install umap-learn\n",
        "!pip install datashader\n",
        "!pip install bokeh\n",
        "!pip install holoviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXr0DgjjvCJa"
      },
      "source": [
        "---\n",
        "VAE Initializaiton, Visualization and Training\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATqtpHxlVJdO"
      },
      "outputs": [],
      "source": [
        "import torch   \n",
        "import torch.nn as nn                          \n",
        "import torch.nn.functional as F                \n",
        "import torch.optim as optim   \n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os                             \n",
        "\n",
        "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
        "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns    \n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go        \n",
        "                \n",
        "from tqdm import tqdm\n",
        "\n",
        "import umap\n",
        "import umap.plot\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#based on sensor data can you determine the stimulus that is currently in use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msgUiu6VVLb2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/WEAR_LAB/Research_Pytorch/S1_E1_A1_v3.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YWNrSbVhwqw"
      },
      "outputs": [],
      "source": [
        "#df = df.drop(columns=['series_id'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLnkrC9o_CzK"
      },
      "outputs": [],
      "source": [
        "#df.describe()\n",
        "#df.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6VzjKYRRBt7"
      },
      "outputs": [],
      "source": [
        "X = df.iloc[:,1:]\n",
        "y = df.iloc[:, 0:1]\n",
        "print(X.shape, type(X), y.shape, type(y))\n",
        "print()\n",
        "#print(y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkM1pDIrRWqM"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XIsEYVmRloI"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA8-K5g_qNvJ"
      },
      "source": [
        "---\n",
        "Visulaize number of lables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GZFlYtuh4g3"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x = 'stimulus', data=df)\n",
        "df.loc[:,'stimulus'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv5NqT6JgAKo"
      },
      "outputs": [],
      "source": [
        "# Grouping\n",
        "stimulus = df.groupby(\"stimulus\")\n",
        "stimulus.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpiqlKfjqDhy"
      },
      "source": [
        "---\n",
        "Visualize Data Disturbutions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sGkzQcdlBZ5"
      },
      "outputs": [],
      "source": [
        "# #distribution of first 16 features\n",
        "\n",
        "\n",
        "# fig, axs = plt.subplots(nrows=11, ncols=4, figsize=(60, 60))\n",
        "# axs = axs.flatten()\n",
        "# index = 0\n",
        "# for k, v in df.items():\n",
        "#   print(f\"[{index +1}] Updating plot\")\n",
        "#   sns.distplot(v, ax=axs[index])\n",
        "#   index += 1\n",
        "#   if index == 43:\n",
        "#     break \n",
        "# plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ6JhVncSg-a"
      },
      "outputs": [],
      "source": [
        "X = df.iloc[:,1:].values\n",
        "y = df.iloc[:, 0:1].values\n",
        "print(X.shape, type(X), y.shape, type(y))\n",
        "\n",
        "# Data Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n",
        "\n",
        "print(f\"X_train size: {len(X_train)} | X_val size: {len(X_val)} | X_test size: {len(X_test)}\")\n",
        "print(f\"y_train size: {len(y_train)} | y_val size: {len(y_val)} | y_test size: {len(y_test)}\")\n",
        "print()\n",
        "print(f\"Training Feature Split: {X_train.shape} | Training Labels { y_train.shape}\")\n",
        "print(f\"Validation Feature Split: {X_val.shape} | Validation Labels { y_val.shape}\")\n",
        "print(f\"Testing Feature Split: {X_test.shape} | Testing Labels { y_test.shape}\")\n",
        "print()\n",
        "\n",
        "#Normalization Data \n",
        "Minmax = preprocessing.MinMaxScaler()\n",
        "#Standardized = preprocessing.StandardScaler()\n",
        "X_train_Minmax= Minmax.fit_transform(X_train)\n",
        "X_val_Minmax = Minmax.transform(X_val)\n",
        "X_test_Minmax = Minmax.transform(X_test)\n",
        "\n",
        "#Convert to numpy then to torch \n",
        "\n",
        "X_train = torch.from_numpy(X_train_Minmax).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "\n",
        "X_val = torch.from_numpy(X_val_Minmax).float()\n",
        "y_val = torch.from_numpy(y_val).float()\n",
        "\n",
        "X_test = torch.from_numpy(X_test_Minmax).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "print(f\"X_train: {type(X_train)} | y_train {type(y_train)}\")\n",
        "print(f\"X_val: {type(X_val)} | y_train {type(y_val)}\")\n",
        "print(f\"X_test: {type(X_test)} | y_test {type(y_test)}\")\n",
        "print()\n",
        "print(f\"Training: {X_train.shape} , { y_train.shape}\")\n",
        "print(f\"Validation: {X_val.shape} , { y_val.shape}\")\n",
        "print(f\"Testing:  {X_test.shape} , { y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7rZppwdVzdJ"
      },
      "outputs": [],
      "source": [
        "class ClassifierDataset(Dataset):\n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "training = ClassifierDataset(X_train, y_train)\n",
        "validating = ClassifierDataset(X_val, y_val)\n",
        "testing = ClassifierDataset(X_test, y_test)\n",
        "\n",
        "x_sample, y_sample = training[0]\n",
        "print(f'elements of x_sample : \\n{x_sample[:10]} \\nand y_sample : \\n{y_sample}')\n",
        "print(f'x_sample shape : {x_sample.shape} and y_sample shape : {y_sample.shape}')\n",
        "\n",
        "###############################################################################################\n",
        "#Hyperparameters\n",
        "latent_dim = 2\n",
        "input_dim= 41\n",
        "hidden_dim= 20\n",
        "output_dim = 41\n",
        "num_epochs= 20\n",
        "batch_size= 100\n",
        "num_classes = 13\n",
        "learning_rate= 0.0001 #3e-4 #Karpathy constant\n",
        "\n",
        "#beta = 1\n",
        "beta = 0.05\n",
        "alpha = 1\n",
        "\n",
        "train_loader = DataLoader(training, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validating, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(testing, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpNx-NK7cwOM"
      },
      "outputs": [],
      "source": [
        "# classification and matrix \n",
        "\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "# knn.fit(X_train,y_train.ravel())\n",
        "# prediction = knn.predict(X_train)\n",
        "# print('With KNN (K=3) accuracy is: ',knn.score(X_test,y_test)) # accuracy\n",
        "# print('Prediction: {}'.format(prediction))\n",
        "# print(\"-------------\\n\")\n",
        "\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# rf = RandomForestClassifier(random_state = 4)\n",
        "# rf.fit(X_train,y_train.ravel())\n",
        "# y_pred = rf.predict(X_test)\n",
        "# cm = confusion_matrix(y_test,y_pred)\n",
        "# print('Confusion matrix: \\n',cm)\n",
        "# print('Classification report: \\n',classification_report(y_test,y_pred))\n",
        "# y_prediciton = rf.predict(X_test)\n",
        "# score = accuracy_score(y_test, y_prediciton)\n",
        "# sns.heatmap(cm,annot=True,fmt=\"d\") \n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UbXfasD8HQn"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):  \n",
        "  def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "    super(VAE,self).__init__()  \n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)  # no labels\n",
        "    self.mu = nn.Linear(hidden_dim, latent_dim)   # mu\n",
        "    self.logvar = nn.Linear(hidden_dim,latent_dim)   # log-var\n",
        "\n",
        "    self.fc3 = nn.Linear(latent_dim, hidden_dim) \n",
        "    self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "    \n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(latent_dim, 13),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def encode(self, x):     \n",
        "#    print(f'encoder {type(x)}')         \n",
        "    z = F.relu(self.fc1(x))\n",
        "    z = torch.tanh(z) \n",
        "    z1 = self.mu(z)               \n",
        "    z2 = self.logvar(z) \n",
        "    return z1, z2                 # (mu, log-var)\n",
        "\n",
        "  def decode(self, x):\n",
        "#    print(f'decoder {type(x)}')\n",
        "    z = F.relu(self.fc3(x))                    \n",
        "    z = torch.sigmoid(self.fc4(z))      # in [0, 1]\n",
        "    #print(f\"z: {z}\")\n",
        "    return z \n",
        "\n",
        "  def forward(self, x):\n",
        "#    print(f'forward {type(x)}')\n",
        "\n",
        "#  Reparamaterize\n",
        "    mu, logvar = self.encode(x)\n",
        "    stdev = torch.exp(0.5 * logvar)\n",
        "    esp = torch.randn_like(stdev)\n",
        "    z_reparmeterized = mu + (esp * stdev)   \n",
        "    #print(f\"z_reparmeterized : {z_reparmeterized}\")      \n",
        "    x_reconstructed = self.decode(z_reparmeterized)\n",
        "    #print(f\"x_reconstructed : {x_reconstructed}\")\n",
        "\n",
        "    classified = self.classifier(z_reparmeterized)\n",
        "\n",
        "    return (x_reconstructed, z_reparmeterized, classified, mu, logvar)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  x = torch.rand(batch_size,input_dim)\n",
        "  vae = VAE(input_dim, hidden_dim, latent_dim)\n",
        "  x_reconstructed, z_reparmeterized, classified, mu, logvar = vae(x)\n",
        "  print(x_reconstructed.shape)\n",
        "  print(mu.shape)\n",
        "  print(logvar.shape)\n",
        "  print(z_reparmeterized.shape)\n",
        "  print(classified.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UoGJdYPZONI"
      },
      "outputs": [],
      "source": [
        "model  = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(model)\n",
        "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
        "classifier_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def accuracy(y_pred, y_act):\n",
        "  y_pred = torch.round(y_pred)\n",
        "  correct = (y_pred == y_act)\n",
        "  acc1 = correct.sum()/len(correct)\n",
        "  acc2 = torch.round(acc1*100)\n",
        "  # print(f\"z_pred: {y_pred} | lable: {y_act} | correct: {correct} | accuracy {acc1} | accuracy {acc2}\")\n",
        "  return acc2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyxFhL0SXN-_"
      },
      "outputs": [],
      "source": [
        "train_losses=[]\n",
        "val_losses=[]\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "\n",
        "\n",
        "dic = dict(latent_space = list(), mu_list=list(), logsig2_list=list(), y=list())\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  #print(f\"zvalue loop begin {z}\")\n",
        "  train_running_loss = 0\n",
        "  train_running_acc = 0\n",
        "#  loop = tqdm(train_loader)\n",
        "  for i, data in enumerate(train_loader):\n",
        "    inputs, labels = data\n",
        "    #print(f'type data: {type(data)}')\n",
        "    #print(f'type inputs: {type(inputs)}')\n",
        "    #print(f'type labels: {type(labels)}')\n",
        "\n",
        "    x_reconstructed, z_reparmeterized, classified, mu, logvar = model(inputs)\n",
        "    #print(x_reconstructed, type(x_reconstructed))\n",
        "    #print(mu, type(mu))\n",
        "    #print(logvar, type(logvar))\n",
        "\n",
        "    reconstruction_loss = loss_fn(x_reconstructed, inputs)\n",
        "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    loss = alpha*reconstruction_loss + kld_loss*beta\n",
        "\n",
        "    acc_train = accuracy(classified, labels)\n",
        "    #print(reconstruction_loss, kld_loss, loss)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_running_loss += loss.item()\n",
        "    train_loss= train_running_loss/len(train_loader)\n",
        "\n",
        "    train_running_acc += acc_train.item()\n",
        "    train_acc = train_running_acc/len(train_loader)\n",
        "\n",
        "  \n",
        "  z_list, means, logvars , labels = list(), list(), list(), list()\n",
        "\n",
        "  #Evaluation\n",
        "  with torch.inference_mode():\n",
        "    val_running_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    for X, Y in val_loader:\n",
        "      #print(labels)\n",
        "      #inputs = torch.autograd.Variable(inputs)\n",
        "      y_pred, z_reparmeterized, classified, mu, logvar = model(X)\n",
        "      v_reconstruction_loss = loss_fn(y_pred, X)\n",
        "      v_kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "      vloss = alpha*v_reconstruction_loss + v_kld_loss*beta\n",
        "      #print(v_reconstruction_loss, v_kld_loss, vloss)\n",
        "\n",
        "      # yhat = torch.max(z.data,1)\n",
        "      # correct+=(yhat==y_test).sum().int()\n",
        "      # accuracy = correct / n_test\n",
        "      # accuracy_list.append(accuracy)\n",
        "  \n",
        "\n",
        "      # val_acc = accuracy(classified, labels)\n",
        "      val_acc = 0\n",
        "\n",
        "      val_running_loss += vloss.item()\n",
        "      val_loss = val_running_loss/len(val_loader)\n",
        "\n",
        "      # log ...\n",
        "      z_list.append(z_reparmeterized.detach())\n",
        "      means.append(mu.detach())\n",
        "      logvars.append(logvar.detach())\n",
        "      labels.append(Y.detach())\n",
        "\n",
        "  dic['latent_space'].append(torch.cat(z_list))\n",
        "  dic['mu_list'].append(torch.cat(means))\n",
        "  dic['logsig2_list'].append(torch.cat(logvars))\n",
        "  dic['y'].append(torch.cat(labels))\n",
        "\n",
        "  print(f\"Epoch: {epoch+1} / {num_epochs} | reconst_loss: {reconstruction_loss:.3f} | kldiv loss: {kld_loss:.5f} | total loss: {train_loss:.3f} | train acc: {train_acc:.3f} ||| Val Loss: {val_loss:.3f} | val acc: {val_acc:.3f}\")\n",
        "  print(\"------------------------------------------------------------------------------------------------------------------\")\n",
        "  #print(f\"Epoch: {epoch+1} / {num_epochs} | reconst_loss: {v_reconstruction_loss:.3f} | kldiv loss: {v_kld_loss:.3f} | Val Loss: {val_loss:.3f}\")\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "  train_accuracy.append(train_acc)\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaK_eSTWACl3"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'VAE_Model.pt') # Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Www1IBIuH4EY"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses,'-o', label=\"Training loss\")\n",
        "plt.plot(val_losses,'-r',  label=\"Validation loss\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('losses')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHWCgtgq6OJd"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_accuracy,'-o', label=\"Training acc\")\n",
        "# plt.plot(val_acc,'-r',  label=\"Validation acc\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KSX5IZj2KZ0"
      },
      "outputs": [],
      "source": [
        "print(\"Latent Space Visualization\")\n",
        "for i in range (num_epochs):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  z_arr = dic['latent_space'][i].cpu().numpy()\n",
        "  y_arr = dic['y'][i].cpu().numpy()\n",
        "\n",
        "  #Experiment 1\n",
        "  plt.scatter(z_arr[:,0], z_arr[:,1], c = y_arr, edgecolor='none', alpha=0.5,\n",
        "              cmap=plt.cm.get_cmap('hsv', 13))\n",
        "  cb = plt.colorbar(ticks=[0,1,2,3,4,5,6,7,8,9,10,11,12],values=[0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "  #Experiment 3\n",
        "  # plt.scatter(z_arr[:,0], z_arr[:,1], c = y_arr, edgecolor='none', alpha=0.5,\n",
        "  #             cmap=plt.cm.get_cmap('hsv', 23))\n",
        "  #cb = plt.colorbar(ticks=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],values=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23])\n",
        "  cb.ax.tick_params(labelsize=10)\n",
        "  plt.xticks(fontsize= 10)\n",
        "  plt.yticks(fontsize= 10)\n",
        "  plt.xlabel('z[0]', fontsize= 10)\n",
        "  plt.ylabel('z[1]', fontsize= 10)\n",
        "  plt.title(f'VAE train dataset with latent space Dim=2  Epoch number: {i+1} ', fontsize= 12)\n",
        "  # plt.show()\n",
        "  plt.close()\n",
        "  fig.savefig(f\"/content/drive/MyDrive/WEAR_LAB/Research_Pytorch/VAE_Images/VAEtrain_images{i:001}\" + \".png\")\n",
        "  print(f\"Latent Space Image {i+1} stored.\")\n",
        "\n",
        "import imageio\n",
        "gif = []\n",
        "for i in range(num_epochs):\n",
        "  each_image = imageio.imread(f\"/content/drive/MyDrive/WEAR_LAB/Research_Pytorch/VAE_Images/VAEtrain_images{i}\" + \".png\")# here read all images\n",
        "  gif.append(each_image)\n",
        "imageio.mimsave(\"/content/result.gif\",gif)\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "fname = '/content/result.gif'\n",
        "Image(open(fname, 'rb').read())  # local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKwtgkkkqljv"
      },
      "outputs": [],
      "source": [
        "# for i in range (num_epochs):\n",
        "#   z_arr = dic['latent_space'][i].cpu().numpy()\n",
        "#   y_arr = dic['y'][i].cpu().numpy()\n",
        "#   plt.figure(figsize = (10,5))\n",
        "#   plt.subplot(1,2,1)\n",
        "#   plt.scatter(z_arr[:,0], z_arr[:,1], c = y_arr)\n",
        "#   plt.colorbar()\n",
        "#   plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpIALMGuu8Kc"
      },
      "source": [
        "---\n",
        "Classificaiton and Transfer\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIqKioogLtgC",
        "outputId": "54bf89cc-3902-445b-9ba5-c4943d84d51c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of model: 0.9689 \n",
            "Accuracy of KNN model : 0.9013\n",
            "PCA Train Accuracy: 100.0%\n",
            "PCA Test Accuracy: 90.5952380952381%\n"
          ]
        }
      ],
      "source": [
        "#Classifiers and PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rand_seed = 7\n",
        "\n",
        "test = DecisionTreeClassifier(random_state=rand_seed)\n",
        "test.fit(X_train, y_train)\n",
        "\n",
        "y_prediciton = test.predict(X_test)\n",
        "score = accuracy_score(y_test, y_prediciton)\n",
        "print(f\"Accuracy of model: {score:.4f} \")\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train.ravel())\n",
        "\n",
        "y_knn_pred = knn.predict(X_test)\n",
        "knn_score = accuracy_score(y_test, y_knn_pred)\n",
        "print(\"Accuracy of KNN model : {:.4f}\".format(knn_score))\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "pca = PCA(n_components=2)  # same size as autoencoder latent space\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1).fit(X_train_pca,  y_train.ravel())\n",
        "print(f'PCA Train Accuracy: {rf.score(X_train_pca, y_train)*100}%')\n",
        "print(f'PCA Test Accuracy: {rf.score(X_test_pca, y_test)*100}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGBe66RJDPVV"
      },
      "outputs": [],
      "source": [
        "y_prediciton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GybBMHWyvDm"
      },
      "source": [
        "---\n",
        "Test\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHFcHMT8uetL"
      },
      "outputs": [],
      "source": [
        "# # working VAE with latent space repersentation\n",
        "\n",
        "# def train_model(beta, epochs, model):\n",
        "#   dic = dict(latent_space = list(), mu_list=list(), logsig2_list=list(), y=list())\n",
        "#   for epoch in range(0, epochs + 1):\n",
        "#     if epoch > 0: \n",
        "#       model.train()\n",
        "#       train_loss =0\n",
        "#       for X, _ in train_loader:\n",
        "#         x_reconstructed, z_reparmeterized, mu, logvar = model(X)\n",
        "\n",
        "#         reconstruction_loss = loss_fn(x_reconstructed, X)\n",
        "#         kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "#         loss = reconstruction_loss + kld_loss*3\n",
        "#         train_loss += loss.item()\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#       print(f'----> Epoch: {epoch} Average loss: {train_loss / len(train_loader):.4f}')\n",
        "    \n",
        "#     # Validation \n",
        "#     z_list, means, logvars , labels = list(), list(), list(), list()\n",
        "#     with torch.no_grad():\n",
        "#         model.eval()\n",
        "#         validate_loss = 0\n",
        "#         for X, Y in test_loader:\n",
        "#           X = X.to(device)\n",
        "#           # forward ...\n",
        "#           x_reconstructed, z_reparmeterized, mu, logvar = model(X)\n",
        "\n",
        "#           reconstruction_loss = loss_fn(x_reconstructed, X)\n",
        "#           kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "#           validation_loss = reconstruction_loss + kld_loss\n",
        "#           validate_loss += validation_loss.item()\n",
        "\n",
        "#           # log ...\n",
        "#           z_list.append(z_reparmeterized.detach())\n",
        "#           means.append(mu.detach())\n",
        "#           logvars.append(logvar.detach())\n",
        "#           labels.append(Y.detach())\n",
        "          \n",
        "\n",
        "#     dic['latent_space'].append(torch.cat(z_list))\n",
        "#     dic['mu_list'].append(torch.cat(means))\n",
        "#     dic['logsig2_list'].append(torch.cat(logvars))\n",
        "#     dic['y'].append(torch.cat(labels))\n",
        "    \n",
        "#     print(f'----> Val loss:  {validate_loss / len(val_loader):.4f}')\n",
        "#   return dic\n",
        "\n",
        "# dic = train_model(beta, num_epochs, model)\n",
        "\n",
        "# z_arr = dic['latent_space'][0].cpu().numpy()\n",
        "# y_arr = dic['y'][0].cpu().numpy()\n",
        "# plt.figure(figsize = (10,5))\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.scatter(z_arr[:,0], z_arr[:,1], c = y_arr)\n",
        "# #plt.scatter(z_arr[:,1], z_arr[:,2], c = y_arr)\n",
        "# plt.colorbar()\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88IM8u95drAw"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwdTKORKKCNR"
      },
      "outputs": [],
      "source": [
        "# class Loss_Term(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(Loss_Term, self).__init__()\n",
        "#     self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
        "#     #self.ce_loss = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "# # x_reconstructed is the recon_batch created in the forward in the model\n",
        "# # x is the original x batch, mu is mu and logvar is logvar\n",
        "#   def forward(self, x_reconstructed,x, mu, logvar):\n",
        "#     #loss_CE = self.ce_loss(x_reconstructed, x)\n",
        "#     loss_MSE = self.mse_loss(x_reconstructed, x)\n",
        "#     loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "#     return loss_MSE + loss_KLD \n",
        "#     #return loss_CE + loss_KLD\n",
        "\n",
        "# loss_func = Loss_Term()\n",
        "\n",
        "# val_losses = []\n",
        "# train_losses = []\n",
        "\n",
        "# def train(epoch):\n",
        "#     model.train()\n",
        "#     train_loss = 0\n",
        "#     for (inputs, labels) in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         recon_batch, mu, logvar = model(inputs)\n",
        "#         loss = loss_func(recon_batch, mu, logvar)\n",
        "#         loss.backward()\n",
        "#         train_loss += loss.item()\n",
        "#         optimizer.step()\n",
        "# #        if batch_idx % log_interval == 0:\n",
        "# #            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "# #                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
        "# #                       100. * batch_idx / len(trainloader),\n",
        "# #                       loss.item() / len(data)))\n",
        "#     if epoch % 200 == 0:        \n",
        "#         print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "#             epoch, train_loss / len(train_loader.dataset)))\n",
        "#         train_losses.append(train_loss / len(train_loader.dataset))\n",
        "# for epoch in range(1, num_epochs + 1):\n",
        "#     train(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bav3pPGdrntA"
      },
      "outputs": [],
      "source": [
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, latent_dims):\n",
        "#         super(Encoder, self).__init__()\n",
        "#         self.linear1 = nn.Linear(41, 20)\n",
        "#         self.linear2 = nn.Linear(20, latent_dims)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = torch.flatten(x, start_dim=1)\n",
        "#         x = F.relu(self.linear1(x))\n",
        "#         return self.linear2(x)\n",
        "\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, latent_dims):\n",
        "#         super(Decoder, self).__init__()\n",
        "#         self.linear1 = nn.Linear(latent_dims, 20)\n",
        "#         self.linear2 = nn.Linear(20, 41)\n",
        "\n",
        "#     def forward(self, z):\n",
        "#         z = F.relu(self.linear1(z))\n",
        "#         z = torch.sigmoid(self.linear2(z))\n",
        "#         return z\n",
        "\n",
        "# class Autoencoder(nn.Module):\n",
        "#     def __init__(self, latent_dims):\n",
        "#         super(Autoencoder, self).__init__()\n",
        "#         self.encoder = Encoder(latent_dims)\n",
        "#         self.decoder = Decoder(latent_dims)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         z = self.encoder(x)\n",
        "#         return self.decoder(z)\n",
        "\n",
        "# losses = []\n",
        "# train_losses=[]\n",
        "# def train(autoencoder, data, epochs=num_epochs):\n",
        "#     optimizer = optim.Adam(params=autoencoder.parameters(), lr=learning_rate)\n",
        "#     for epoch in range(epochs):\n",
        "#         running_loss = 0\n",
        "#         for x, y in data:\n",
        "#             x = x.to(device) # GPU\n",
        "#             optimizer.zero_grad()\n",
        "#             x_hat = autoencoder(x)\n",
        "#             loss = ((x - x_hat)**2).sum()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "#             train_loss=running_loss/len(train_loader)\n",
        "#             i= 0\n",
        "#         print(f\"Epoch: {epoch+1} / {epochs} | total loss: {(loss)}\")\n",
        "#         train_losses.append(train_loss)\n",
        "        \n",
        "#         plt.plot(train_losses,'-o')\n",
        "#         plt.xlabel('epoch')\n",
        "#         plt.ylabel('losses')\n",
        "#         plt.title('Train Losses')\n",
        "#         plt.show()  \n",
        "\n",
        "#         # if i % 200 == 0:\n",
        "#         #       print(epoch)\n",
        "\n",
        "\n",
        "#     return autoencoder\n",
        "\n",
        "# latent_dims = 2\n",
        "# autoencoder = Autoencoder(latent_dims).to(device) # GPU\n",
        "\n",
        "# data = train_loader\n",
        "\n",
        "# autoencoder = train(autoencoder, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3umbCo2stCwe"
      },
      "outputs": [],
      "source": [
        "# def plot_latent(autoencoder, data, num_batches=100):\n",
        "#     for i, (x, y) in enumerate(data):\n",
        "#         z = autoencoder.encoder(x.to(device))\n",
        "#         z = z.to('cpu').detach().numpy()\n",
        "#         plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')\n",
        "#         if i > num_batches:\n",
        "#             plt.colorbar()\n",
        "#             break\n",
        "# plot_latent(autoencoder, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Avz6DsksLM"
      },
      "source": [
        "---\n",
        "Example VAE\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa9wexZAku9X"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torchvision\n",
        "# from torch import nn\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision import transforms\n",
        "# from torchvision.datasets import MNIST\n",
        "# from matplotlib import pyplot as plt\n",
        "# # Displaying routine\n",
        "\n",
        "# def display_images(in_, out, n=1, label=None, count=False):\n",
        "#     for N in range(n):\n",
        "#         if in_ is not None:\n",
        "#             in_pic = in_.data.cpu().view(-1, 28, 28)\n",
        "#             plt.figure(figsize=(18, 4))\n",
        "#             plt.suptitle(label + ' – real test data / reconstructions', color='w', fontsize=16)\n",
        "#             for i in range(4):\n",
        "#                 plt.subplot(1,4,i+1)\n",
        "#                 plt.imshow(in_pic[i+4*N])\n",
        "#                 plt.axis('off')\n",
        "#         out_pic = out.data.cpu().view(-1, 28, 28)\n",
        "#         plt.figure(figsize=(18, 6))\n",
        "#         for i in range(4):\n",
        "#             plt.subplot(1,4,i+1)\n",
        "#             plt.imshow(out_pic[i+4*N])\n",
        "#             plt.axis('off')\n",
        "#             if count: plt.title(str(4 * N + i), color='w')\n",
        "# # Set random seeds\n",
        "\n",
        "# torch.manual_seed(1)\n",
        "# torch.cuda.manual_seed(1)\n",
        "# # Define data loading step\n",
        "\n",
        "# batch_size = 256\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     MNIST('./data', train=True, download=True,\n",
        "#                    transform=transforms.ToTensor()),\n",
        "#     batch_size=batch_size, shuffle=True, **kwargs)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
        "#     batch_size=batch_size, shuffle=True, **kwargs)\n",
        "# # Defining the device\n",
        "\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# # Defining the model\n",
        "\n",
        "# d = 20\n",
        "\n",
        "# class VAE(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.encoder = nn.Sequential(\n",
        "#             nn.Linear(784, d ** 2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(d ** 2, d * 2)\n",
        "#         )\n",
        "\n",
        "#         self.decoder = nn.Sequential(\n",
        "#             nn.Linear(d, d ** 2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(d ** 2, 784),\n",
        "#             nn.Sigmoid(),\n",
        "#         )\n",
        "\n",
        "#     def reparameterise(self, mu, logvar):\n",
        "#         if self.training:\n",
        "#             std = logvar.mul(0.5).exp_()\n",
        "#             eps = std.new_empty(std.size()).normal_()\n",
        "#             return eps.mul_(std).add_(mu)\n",
        "#         else:\n",
        "#             return mu\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         mu_logvar = self.encoder(x.view(-1, 784)).view(-1, 2, d)\n",
        "#         mu = mu_logvar[:, 0, :]\n",
        "#         logvar = mu_logvar[:, 1, :]\n",
        "#         z = self.reparameterise(mu, logvar)\n",
        "#         return self.decoder(z), mu, logvar\n",
        "\n",
        "# model = VAE().to(device)\n",
        "# # Setting the optimiser\n",
        "\n",
        "# learning_rate = 1e-3\n",
        "\n",
        "# optimizer = torch.optim.Adam(\n",
        "#     model.parameters(),\n",
        "#     lr=learning_rate,\n",
        "# )\n",
        "# # Reconstruction + β * KL divergence losses summed over all elements and batch\n",
        "\n",
        "# def loss_function(x_hat, x, mu, logvar, β=1):\n",
        "#     BCE = nn.functional.binary_cross_entropy(\n",
        "#         x_hat, x.view(-1, 784), reduction='sum'\n",
        "#     )\n",
        "#     KLD = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
        "\n",
        "#     return BCE + β * KLD\n",
        "# # Training and testing the VAE\n",
        "\n",
        "# epochs = 10\n",
        "# codes = dict(μ=list(), logσ2=list(), y=list())\n",
        "# for epoch in range(0, epochs + 1):\n",
        "#     # Training\n",
        "#     if epoch > 0:  # test untrained net first\n",
        "#         model.train()\n",
        "#         train_loss = 0\n",
        "#         for x, _ in train_loader:\n",
        "#             x = x.to(device)\n",
        "#             # ===================forward=====================\n",
        "#             x_hat, mu, logvar = model(x)\n",
        "#             loss = loss_function(x_hat, x, mu, logvar)\n",
        "#             train_loss += loss.item()\n",
        "#             # ===================backward====================\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#         # ===================log========================\n",
        "#         print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
        "    \n",
        "#     # Testing\n",
        "    \n",
        "#     means, logvars, labels = list(), list(), list()\n",
        "#     with torch.no_grad():\n",
        "#         model.eval()\n",
        "#         test_loss = 0\n",
        "#         for x, y in test_loader:\n",
        "#             x = x.to(device)\n",
        "#             # ===================forward=====================\n",
        "#             x_hat, mu, logvar = model(x)\n",
        "#             test_loss += loss_function(x_hat, x, mu, logvar).item()\n",
        "#             # =====================log=======================\n",
        "#             means.append(mu.detach())\n",
        "#             logvars.append(logvar.detach())\n",
        "#             labels.append(y.detach())\n",
        "#     # ===================log========================\n",
        "#     codes['μ'].append(torch.cat(means))\n",
        "#     codes['logσ2'].append(torch.cat(logvars))\n",
        "#     codes['y'].append(torch.cat(labels))\n",
        "#     test_loss /= len(test_loader.dataset)\n",
        "#     print(f'====> Test set loss: {test_loss:.4f}')\n",
        "#     display_images(x, x_hat, 1, f'Epoch {epoch}')\n",
        "# # Generating a few samples\n",
        "\n",
        "# N = 16\n",
        "# z = torch.randn((N, d)).to(device)\n",
        "# sample = model.decoder(z)\n",
        "# display_images(None, sample, N // 4, count=True)\n",
        "# # Display last test batch\n",
        "\n",
        "# display_images(None, x, 4, count=True)\n",
        "# # Choose starting and ending point for the interpolation -> shows original and reconstructed\n",
        "\n",
        "# A, B = 1, 14\n",
        "# sample = model.decoder(torch.stack((mu[A].data, mu[B].data), 0))\n",
        "# display_images(None, torch.stack(((\n",
        "#     x[A].data.view(-1),\n",
        "#     x[B].data.view(-1),\n",
        "#     sample.data[0],\n",
        "#     sample.data[1]\n",
        "# )), 0))\n",
        "# # Perform an interpolation between input A and B, in N steps\n",
        "\n",
        "# N = 16\n",
        "# code = torch.Tensor(N, 20).to(device)\n",
        "# sample = torch.Tensor(N, 28, 28).to(device)\n",
        "# for i in range(N):\n",
        "#     code[i] = i / (N - 1) * mu[B].data + (1 - i / (N - 1) ) * mu[A].data\n",
        "#     # sample[i] = i / (N - 1) * x[B].data + (1 - i / (N - 1) ) * x[A].data\n",
        "# sample = model.decoder(code)\n",
        "# display_images(None, sample, N // 4, count=True)\n",
        "# import numpy as np\n",
        "# from sklearn.manifold import TSNE\n",
        "# from res.plot_lib import set_default\n",
        "# set_default(figsize=(15, 4))\n",
        "# X, Y, E = list(), list(), list()  # input, classes, embeddings\n",
        "# N = 1000  # samples per epoch\n",
        "# epochs = (0, 5, 10)\n",
        "# for epoch in epochs:\n",
        "#     X.append(codes['μ'][epoch][:N])\n",
        "#     E.append(TSNE(n_components=2).fit_transform(X[-1].detach().cpu()))\n",
        "#     Y.append(codes['y'][epoch][:N])\n",
        "# f, a = plt.subplots(ncols=3)\n",
        "# for i, e in enumerate(epochs):\n",
        "#     s = a[i].scatter(E[i][:,0], E[i][:,1], c=Y[i], cmap='tab10')\n",
        "#     a[i].grid(False)\n",
        "#     a[i].set_title(f'Epoch {e}')\n",
        "#     a[i].axis('equal')\n",
        "# f.colorbar(s, ax=a[:], ticks=np.arange(10), boundaries=np.arange(11) - .5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "M8iJEJRDVWqt",
        "1GybBMHWyvDm"
      ],
      "provenance": [],
      "mount_file_id": "1ZW9qvT8FiWFe8uZmqDH_K7Dn5Q-LXb0a",
      "authorship_tag": "ABX9TyOd+aOSc+yeGuCrxo/t7u1c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}